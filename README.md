# ğŸ§ª HF Distiller â€” Knowledge Distillation for Hugging Face Models

![HF Banner](https://huggingface.co/front/assets/huggingface_logo.svg)

[![Python](https://img.shields.io/badge/python-3.9%2B-blue)](https://www.python.org/)
[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)
[![Hugging Face](https://img.shields.io/badge/huggingface-Dhiraj309-orange)](https://huggingface.co/Dhiraj309)

**HF Distiller** is an **open-source toolkit** for performing **knowledge distillation** on Hugging Face Transformers models. It allows developers to **train smaller, faster student models** from large pre-trained teacher models while maintaining high performance.

---

## ğŸ“– Overview

Knowledge Distillation (KD) compresses a large model into a smaller one by transferring the â€œknowledgeâ€ learned by the teacher to the student. HF Distiller wraps around Hugging Faceâ€™s `Trainer` to make KD **accessible, modular, and intuitive**.

**Key Features:**

* âœ… Load any teacher model from Hugging Face Hub
* âœ… Create smaller student models from scratch
* âœ… Supports Hugging Face tokenizers
* âœ… Seamless integration with the `datasets` library
* âœ… Transparent logging and checkpointing
* âœ… Fully compatible with PyTorch and Transformers

---

## ğŸ–¼ Architecture

```text
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚      Teacher Model      â”‚  Pretrained Hugging Face LM
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Knowledge Distillation  â”‚  Transfer teacher knowledge + KD loss
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚      Student Model      â”‚  Smaller, efficient model trained from scratch
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš¡ Installation

```bash
#Install transformers_distilattion (Recommended)
pip install --no-deps git+https://github.com/Dhiraj309/transformers_distillation.git

#OR

# Clone repository
git clone https://github.com/Dhiraj309/hf_distiller.git
cd hf_distiller

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸƒ Quick Start

```python
from hf_distiller.models import load_teacher, load_student
from hf_distiller.trainer import DistillTrainer
from transformers import AutoTokenizer, TrainingArguments
from datasets import Dataset

# Example dataset
dataset = Dataset.from_dict({"text": ["Hello world!", "AI is amazing."]})

# Load teacher
teacher = load_teacher("google-bert/bert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")

# Create student model
student = load_student(
    model_name_or_path="google-bert/bert-base-uncased",
    from_scratch=True,
    n_layers=4,
    n_heads=4,
    n_embd=256,
    is_pretrained=False
)

# Tokenize
def tokenize(batch):
    return tokenizer(batch["text"], max_length=128, padding=True, truncation=True)

tokenized = dataset.map(tokenize, remove_columns=["text"])

# Training arguments
training_args = TrainingArguments(
    output_dir="./student-llm",
    per_device_train_batch_size=1,
    num_train_epochs=1,
    learning_rate=2e-4,
    report_to="none"
)

# Train student with KD
trainer = DistillTrainer(
    teacher_model=teacher,
    student_model=student,
    train_dataset=tokenized,
    tokenizer=tokenizer,
    training_args=training_args,
    kd_alpha=0.5,
    temperature=2.0
)
trainer.train()
```

---

## ğŸ“‚ Project Status

| Stage                | Status         |
| -------------------- | -------------- |
| Core Development     | âœ… Complete     |
| Documentation        | âœ… Complete     |
| Community Feedback   | ğŸš§ In Progress |
| Tutorials & Examples | ğŸš§ In Progress |

---

## ğŸ¤ Collaboration

We welcome contributions from the community, including:

* Pull requests for new KD strategies
* Bug reports and feature requests
* Tutorials and example scripts
* Optimization for faster student training

ğŸ”— GitHub: [Dhiraj309](https://github.com/Dhiraj309)
ğŸ”— Hugging Face: [Dhiraj309](https://huggingface.co/Dhiraj309)

---

## ğŸ“œ License

Released under the **MIT License** â€” free to use, modify, and distribute. See [LICENSE](LICENSE) for full terms.
